{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取节点的Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.007599</td>\n",
       "      <td>-0.007655</td>\n",
       "      <td>-0.003703</td>\n",
       "      <td>-0.007203</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>-0.008866</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005631</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>-0.013618</td>\n",
       "      <td>-0.008738</td>\n",
       "      <td>0.007777</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>-0.015584</td>\n",
       "      <td>-0.005586</td>\n",
       "      <td>0.004156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.007599</td>\n",
       "      <td>-0.007655</td>\n",
       "      <td>-0.003703</td>\n",
       "      <td>-0.007203</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>-0.008866</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005631</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>-0.013618</td>\n",
       "      <td>-0.008738</td>\n",
       "      <td>0.007777</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>-0.015584</td>\n",
       "      <td>-0.005586</td>\n",
       "      <td>0.004156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.007599</td>\n",
       "      <td>-0.007655</td>\n",
       "      <td>-0.003703</td>\n",
       "      <td>-0.007203</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>-0.008866</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005631</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>-0.013618</td>\n",
       "      <td>-0.008738</td>\n",
       "      <td>0.007777</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>-0.015584</td>\n",
       "      <td>-0.005586</td>\n",
       "      <td>0.004156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>-0.007599</td>\n",
       "      <td>-0.007655</td>\n",
       "      <td>-0.003703</td>\n",
       "      <td>-0.007203</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.006483</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>-0.008866</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005631</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>-0.013618</td>\n",
       "      <td>-0.008738</td>\n",
       "      <td>0.007777</td>\n",
       "      <td>0.005095</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>-0.015584</td>\n",
       "      <td>-0.005586</td>\n",
       "      <td>0.004156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.180584</td>\n",
       "      <td>0.526513</td>\n",
       "      <td>0.793145</td>\n",
       "      <td>-0.245426</td>\n",
       "      <td>-0.220634</td>\n",
       "      <td>0.559357</td>\n",
       "      <td>1.059898</td>\n",
       "      <td>0.148331</td>\n",
       "      <td>-0.981740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003815</td>\n",
       "      <td>-0.553314</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.335591</td>\n",
       "      <td>0.143528</td>\n",
       "      <td>-1.134055</td>\n",
       "      <td>0.629610</td>\n",
       "      <td>0.330191</td>\n",
       "      <td>0.377519</td>\n",
       "      <td>-0.798777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>1614</td>\n",
       "      <td>0.115819</td>\n",
       "      <td>0.086158</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.086158</td>\n",
       "      <td>0.074859</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.007062</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>1615</td>\n",
       "      <td>0.153169</td>\n",
       "      <td>0.107394</td>\n",
       "      <td>0.017606</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.096831</td>\n",
       "      <td>0.056338</td>\n",
       "      <td>0.012324</td>\n",
       "      <td>0.012324</td>\n",
       "      <td>0.019366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>1616</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.111667</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.106667</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1617</th>\n",
       "      <td>1617</td>\n",
       "      <td>0.131970</td>\n",
       "      <td>0.105948</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.098513</td>\n",
       "      <td>0.053903</td>\n",
       "      <td>0.020446</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.014870</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005576</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.007435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>1618</td>\n",
       "      <td>0.040462</td>\n",
       "      <td>0.023121</td>\n",
       "      <td>0.023121</td>\n",
       "      <td>0.046243</td>\n",
       "      <td>0.046243</td>\n",
       "      <td>0.046243</td>\n",
       "      <td>0.011561</td>\n",
       "      <td>0.023121</td>\n",
       "      <td>0.028902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011561</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.017341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005780</td>\n",
       "      <td>0.011561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1619 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6   \\\n",
       "0        0 -0.007599 -0.007655 -0.003703 -0.007203  0.002032  0.006483   \n",
       "1        1 -0.007599 -0.007655 -0.003703 -0.007203  0.002032  0.006483   \n",
       "2        2 -0.007599 -0.007655 -0.003703 -0.007203  0.002032  0.006483   \n",
       "3        3 -0.007599 -0.007655 -0.003703 -0.007203  0.002032  0.006483   \n",
       "4        4  1.180584  0.526513  0.793145 -0.245426 -0.220634  0.559357   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "1614  1614  0.115819  0.086158  0.011299  0.008475  0.086158  0.074859   \n",
       "1615  1615  0.153169  0.107394  0.017606  0.007042  0.096831  0.056338   \n",
       "1616  1616  0.120000  0.111667  0.020000  0.008333  0.106667  0.060000   \n",
       "1617  1617  0.131970  0.105948  0.009294  0.020446  0.098513  0.053903   \n",
       "1618  1618  0.040462  0.023121  0.023121  0.046243  0.046243  0.046243   \n",
       "\n",
       "            7         8         9   ...        55        56        57  \\\n",
       "0     0.005779 -0.008866 -0.004960  ... -0.005631  0.002488 -0.013618   \n",
       "1     0.005779 -0.008866 -0.004960  ... -0.005631  0.002488 -0.013618   \n",
       "2     0.005779 -0.008866 -0.004960  ... -0.005631  0.002488 -0.013618   \n",
       "3     0.005779 -0.008866 -0.004960  ... -0.005631  0.002488 -0.013618   \n",
       "4     1.059898  0.148331 -0.981740  ...  0.003815 -0.553314  0.003378   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1614  0.011299  0.007062  0.012712  ...  0.001412  0.002825  0.002825   \n",
       "1615  0.012324  0.012324  0.019366  ...  0.000000  0.000000  0.003521   \n",
       "1616  0.018333  0.008333  0.018333  ...  0.001667  0.000000  0.001667   \n",
       "1617  0.020446  0.007435  0.014870  ...  0.005576  0.001859  0.007435   \n",
       "1618  0.011561  0.023121  0.028902  ...  0.005780  0.000000  0.017341   \n",
       "\n",
       "            58        59        60        61        62        63        64  \n",
       "0    -0.008738  0.007777  0.005095  0.003780 -0.015584 -0.005586  0.004156  \n",
       "1    -0.008738  0.007777  0.005095  0.003780 -0.015584 -0.005586  0.004156  \n",
       "2    -0.008738  0.007777  0.005095  0.003780 -0.015584 -0.005586  0.004156  \n",
       "3    -0.008738  0.007777  0.005095  0.003780 -0.015584 -0.005586  0.004156  \n",
       "4     0.335591  0.143528 -1.134055  0.629610  0.330191  0.377519 -0.798777  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1614  0.000000  0.001412  0.002825  0.002825  0.001412  0.002825  0.000000  \n",
       "1615  0.000000  0.000000  0.001761  0.005282  0.000000  0.000000  0.000000  \n",
       "1616  0.001667  0.000000  0.000000  0.005000  0.001667  0.000000  0.003333  \n",
       "1617  0.000000  0.003717  0.000000  0.001859  0.001859  0.001859  0.000000  \n",
       "1618  0.000000  0.011561  0.005780  0.017341  0.000000  0.005780  0.011561  \n",
       "\n",
       "[1619 rows x 65 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows',10) # 调整pandas行的显示限制 \n",
    "node_features = pd.read_csv('data/Drug_Protein_Attribute.csv',header = None) \n",
    "node_features  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将词向量提取为特征,第二列到倒数第一列\n",
    "node_features = node_features.iloc[:,1:]\n",
    " # 检查特征：共64个特征，837个样本点\n",
    "print(node_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 划分训练集测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTIs= pd.read_csv('data/Drug_Protein_Num.csv',header = None) \n",
    "DTIs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 生成随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "def partition(ls, size):\n",
    "    \"\"\"\n",
    "    Returns a new list with elements\n",
    "    of which is a list of certain size.\n",
    "\n",
    "        >>> partition([1, 2, 3, 4], 3)\n",
    "        [[1, 2, 3], [4]]\n",
    "    \"\"\"\n",
    "    return [ls[i:i+size] for i in range(0, len(ls), size)]\n",
    "# 由AllEdge产生RandomList\n",
    "RandomList = random.sample(range(0, len(DTIs)), len(DTIs))\n",
    "print('len(RandomList)', len(RandomList))\n",
    "NewRandomList = partition(RandomList, math.ceil(len(RandomList) / 5))\n",
    "print('len(NewRandomList[0])', len(NewRandomList[0]))\n",
    "#NaN = pd.isnull(NewRandomList).any(0).nonzero()[0]\n",
    "NewRandomList = pd.DataFrame(NewRandomList)\n",
    "NewRandomList = NewRandomList.fillna(int(0))\n",
    "NewRandomList = NewRandomList.astype(int)\n",
    "NewRandomList.to_csv('data/NewRandomList.csv', header=None,index=False)\n",
    "#del NewRandomList, RandomList\n",
    "NewRandomList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nindex = pd.read_csv('data/NewRandomList.csv',header=None)\n",
    "for i in range(len(Nindex)):\n",
    "    kk = []\n",
    "    for j in range(5):\n",
    "        if j !=i:\n",
    "            kk.append(j)\n",
    "    index = np.hstack([np.array(Nindex)[kk[0]],np.array(Nindex)[kk[1]],np.array(Nindex)[kk[2]],np.array(Nindex)[kk[3]]])\n",
    "    DTIs_train= pd.DataFrame(np.array(DTIs)[index])\n",
    "    DTIs_train.to_csv('data/DTIs_train'+str(i)+'.csv', header=None,index=False)\n",
    "    DTIs_train = DTIs_train.sample(frac=1.0)\n",
    "    DTIs_train.to_csv('data/DTIs_train'+str(i)+'.txt', sep='\\t' ,header=None,index=False)\n",
    "    DTIs_test=pd.DataFrame(np.array(DTIs)[np.array(Nindex)[i]])\n",
    "    DTIs_test.to_csv('data/DTIs_test'+str(i)+'.csv', header=None,index=False)\n",
    "    print(i)\n",
    "del Nindex, index, DTIs_train, DTIs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置节点类型标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(np.random.rand(max(DTIs[1])+1,1))#pd.DataFrame(np.random.rand(max(NLMI_num),1))\n",
    "labels[0:max(DTIs[0])+1]=0\n",
    "labels[max(DTIs[0])+1:]=1\n",
    "labels = labels[0] # 提取节点标签列\n",
    "labels.to_csv('data/labels.csv',header=0,index=0)\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成负样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NegativeGenerate(LncDisease, AllRNA,AllDisease):\n",
    "    # 负样本为全部的disease-rna（328*881）中随机抽取，未在内LncDisease即为负样本\n",
    "    import random\n",
    "    NegativeSample = []\n",
    "    counterN = 0\n",
    "    while counterN < len(LncDisease):  # 随机选出一个疾病rna对\n",
    "        counterR = random.randint(0, len(AllRNA) - 1)\n",
    "        counterD = random.randint(0, len(AllDisease) - 1)\n",
    "        DiseaseAndRnaPair = []\n",
    "        DiseaseAndRnaPair.append(AllRNA[counterR])\n",
    "        DiseaseAndRnaPair.append(AllDisease[counterD])\n",
    "        flag1 = 0\n",
    "        counter = 0\n",
    "        while counter < len(LncDisease):\n",
    "            if DiseaseAndRnaPair == LncDisease[counter]:\n",
    "                flag1 = 1\n",
    "                break\n",
    "            counter = counter + 1\n",
    "        if flag1 == 1:\n",
    "            continue\n",
    "        flag2 = 0\n",
    "        counter1 = 0\n",
    "        while counter1 < len(NegativeSample):  # 在已选的负样本中没有，防止重复\n",
    "            if DiseaseAndRnaPair == NegativeSample[counter1]:\n",
    "                flag2 = 1\n",
    "                break\n",
    "            counter1 = counter1 + 1\n",
    "        if flag2 == 1:\n",
    "            continue\n",
    "        if (flag1 == 0 & flag2 == 0):\n",
    "            NamePair = []  # 生成对\n",
    "            NamePair.append(AllRNA[counterR])\n",
    "            NamePair.append(AllDisease[counterD])\n",
    "            NegativeSample.append(NamePair)\n",
    "            counterN = counterN + 1\n",
    "    return NegativeSample\n",
    "Dr = pd.read_csv('data/drug_name.csv',header=0,names=['id','name'])\n",
    "Pr = pd.read_csv('data/protein_name.csv',header=0,names=['id','name'])\n",
    "NegativeSample = NegativeGenerate(DTIs.values.tolist(),Dr['id'].values.tolist(),Pr['id'].values.tolist())\n",
    "NegativeSample = pd.DataFrame(NegativeSample)\n",
    "NegativeSample.to_csv('data/NegativeSample.csv', header=None,index=False)\n",
    "NegativeSample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立邻接矩阵方阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建初始的邻接矩阵 #('./data/DDI_train'+str(i)+'.csv')\n",
    "def load_file_as_Adj_matrix(filename):\n",
    "    import scipy.sparse as sp\n",
    "    DTIs_train = pd.read_csv(filename,header=None)\n",
    "    if max(DTIs_train[1]) != 1618:\n",
    "        relation_matrix = np.zeros((1618+1,1618+1))\n",
    "    else:\n",
    "        relation_matrix = np.zeros((max(DTIs_train[1]+1),max(DTIs_train[1]+1)))\n",
    "    for i, j in np.array(DTIs_train):\n",
    "        lnc, mi = int(i), int(j)\n",
    "        relation_matrix[lnc, mi] = 1\n",
    "    Adj = sp.csr_matrix(relation_matrix, dtype=np.float32)\n",
    "    return Adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建GCN训练需要的邻接方阵\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "def load_data(adj,node_features,node_labels):\n",
    "  features = sp.csr_matrix(node_features, dtype=np.float32)  # 储存为csr型稀疏矩阵\n",
    "  # build symmetric adjacency matrix   论文里A^=(D~)^0.5 A~ (D~)^0.5这个公式\n",
    "  # adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "  # 对于无向图，邻接矩阵是对称的。上一步得到的adj是按有向图构建的，转换成无向图的邻接矩阵需要扩充成对称矩阵\n",
    "  features = normalize(features)\n",
    "  adj = normalize(adj + sp.eye(adj.shape[0]))   # eye创建单位矩阵，第一个参数为行数，第二个为列数\n",
    "  # 对应公式A~=A+IN\n",
    "  # 分别构建训练集、验证集、测试集，并创建特征矩阵、标签向量和邻接矩阵的tensor，用来做模型的输入\n",
    "  idx_train = range(500)\n",
    "  idx_val = range(500, 660)\n",
    "  idx_test = range(660, int(adj.shape[0]))  \n",
    "  features = torch.FloatTensor(np.array(features.todense()))  # tensor为pytorch常用的数据结构\n",
    "  labels = torch.LongTensor(np.array(node_labels))\n",
    "  adj = sparse_mx_to_torch_sparse_tensor(adj)   # 邻接矩阵转为tensor处理\n",
    "  idx_train = torch.LongTensor(idx_train)\n",
    "  idx_val = torch.LongTensor(idx_val)\n",
    "  idx_test = torch.LongTensor(idx_test)\n",
    "  return adj, features, labels, idx_train, idx_val, idx_test  \n",
    "def normalize(mx):\n",
    "  \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "  rowsum = np.array(mx.sum(1))  # 对每一行求和\n",
    "  r_inv = np.power(rowsum, -1).flatten()  # 求倒数\n",
    "  r_inv[np.isinf(r_inv)] = 0.  # 如果某一行全为0，则r_inv算出来会等于无穷大，将这些行的r_inv置为0\n",
    "  r_mat_inv = sp.diags(r_inv)  # 构建对角元素为r_inv的对角矩阵\n",
    "  mx = r_mat_inv.dot(mx)\n",
    "  # 用对角矩阵与原始矩阵的点积起到标准化的作用，原始矩阵中每一行元素都会与对应的r_inv相乘，最终相当于除以了sum\n",
    "  return mx\n",
    "\n",
    "def accuracy(output, labels):\n",
    "  preds = output.max(1)[1].type_as(labels) # 使用type_as(tesnor)将张量转换为给定类型的张量。\n",
    "  correct = preds.eq(labels).double()  # 记录等于preds的label eq:equal\n",
    "  correct = correct.sum()\n",
    "  return correct / len(labels)\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):    # 把一个sparse matrix转为torch稀疏张量\n",
    "  \"\"\"\n",
    "  numpy中的ndarray转化成pytorch中的tensor : torch.from_numpy()\n",
    "  pytorch中的tensor转化成numpy中的ndarray : numpy()\n",
    "  \"\"\"\n",
    "  sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "  indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "  # 不懂的可以去看看COO性稀疏矩阵的结构\n",
    "  values = torch.from_numpy(sparse_mx.data)\n",
    "  shape = torch.Size(sparse_mx.shape)\n",
    "  return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "class GraphConvolution(Module):\n",
    "\n",
    "    # 初始化层：输入feature，输出feature，权重，偏移\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))  # FloatTensor建立tensor\n",
    "        # 常见用法self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))：\n",
    "        # 首先可以把这个函数理解为类型转换函数，将一个不可训练的类型Tensor转换成可以训练的类型parameter并将这个parameter\n",
    "        # 绑定到这个module里面，所以经过类型转换这个self.v变成了模型的一部分，成为了模型中根据训练可以改动的参数了。\n",
    "        # 使用这个函数的目的也是想让某些变量在学习的过程中不断的修改其值以达到最优化。\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            # Parameters与register_parameter都会向parameters写入参数，但是后者可以支持字符串命名\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # 初始化权重\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        # size()函数主要是用来统计矩阵元素个数，或矩阵某一维上的元素个数的函数  size（1）为行\n",
    "        self.weight.data.uniform_(-stdv, stdv)  # uniform() 方法将随机生成下一个实数，它在 [x, y] 范围内\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    '''\n",
    "    前馈运算 即计算A~ X W(0)\n",
    "    input X与权重W相乘，然后adj矩阵与他们的积稀疏乘\n",
    "    直接输入与权重之间进行torch.mm操作，得到support，即XW\n",
    "    support与adj进行torch.spmm操作，得到output，即AXW选择是否加bias\n",
    "    '''\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)\n",
    "        # torch.mm(a, b)是矩阵a和b矩阵相乘，torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等\n",
    "        output = torch.spmm(adj, support)\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias \n",
    "        else:\n",
    "            return output\n",
    "#通过设置断点，可以看出output的形式是0.01，0.01，0.01，0.01，0.01，#0.01，0.94]，里面的值代表该x对应标签不同的概率，故此值可转换为#[0,0,0,0,0,0,1]，对应我们之前把标签onthot后的第七种标签\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class GCN(nn.Module):\n",
    "  def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "    # 底层节点的参数，feature的个数；隐层节点个数；最终的分类数\n",
    "    super(GCN, self).__init__()  #  super()._init_()在利用父类里的对象构造函数\n",
    "    self.gc1 = GraphConvolution(nfeat, nhid)   # gc1输入尺寸nfeat，输出尺寸nhid\n",
    "    self.gc2 = GraphConvolution(nhid, nclass)  # gc2输入尺寸nhid，输出尺寸ncalss\n",
    "    self.dropout = dropout\n",
    "    self.weight = Parameter(torch.FloatTensor(nfeat, nhid))  # FloatTensor建立tensor\n",
    "    # 输入分别是特征和邻接矩阵。最后输出为输出层做log_softmax变换的结果\n",
    "  def forward(self, x, adj):\n",
    "    x = F.relu(self.gc1(x, adj))   # adj即公式Z=softmax(A~Relu(A~XW(0))W(1))中的A~\n",
    "    x2 = F.dropout(x, self.dropout, training = self.training)  # x要dropout\n",
    "    x2 = self.gc2(x2, adj)\n",
    "    return F.log_softmax(x2, dim = 1), x2   #, x  # 参数dim=1表示对每一行求softmax，那么每一行的值加起来都等于1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用GCN训练属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "learning_rate = 0.001\n",
    "weight_decay = 5e-4\n",
    "epoch_num = 200\n",
    "dropout = 0.02\n",
    "#in_size = node_features  #设置输入层的维数\n",
    "hi_size = 64 # 16 #设置隐藏层的维数\n",
    "\n",
    "name = locals() # 利用locals()创建变量\n",
    "for i in range(5):\n",
    "    Adj = load_file_as_Adj_matrix('data/DTIs_train'+str(i)+'.csv')\n",
    "    adj, train_features, trian_labels, idx_train, idx_val, idx_test = load_data(Adj,node_features,labels)\n",
    "    model = GCN(nfeat=train_features.shape[1],\n",
    "            nhid=hi_size,\n",
    "            nclass= 16,#labels.max().item() + 1,\n",
    "            dropout=dropout)\n",
    "    model.train()\n",
    "    global Emdebding_train, output\n",
    "    output, Emdebding_train = model(train_features, adj)\n",
    "    Emdebding_GCN = pd.DataFrame(Emdebding_train.detach().numpy())\n",
    "    GCN_A = pd.concat([Emdebding_GCN, node_features], axis=1)\n",
    "    GCN_A.to_csv('data/Emdebding_GCN_1'+str(i)+'.csv', header=None,index=False)\n",
    "    del Adj, adj, train_features, trian_labels, idx_train, idx_val, idx_test, output\n",
    "    print(i)\n",
    "GCN_A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame(np.random.randint(0,1,node_features.shape[0]))\n",
    "dd['index']=dd.index\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成最终训练的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "creat_var = locals() # 利用locals()创建变量\n",
    "creat_var = locals() # 利用locals()创建变量\n",
    "Negative = pd.read_csv('data/NegativeSample.csv',header=None)\n",
    "Nindex = pd.read_csv('data/NewRandomList.csv',header=None)\n",
    "Negative[2] = Negative.apply(lambda x: 0 if x[0] < 0 else 0, axis=1)\n",
    "for i in range(5):\n",
    "    data_train_feature, data_test_feature = [], []\n",
    "    Embedding_GCN = pd.read_csv('data/Emdebding_GCN_1'+str(i)+'.csv',header=None)\n",
    "    Embedding_grarep = pd.read_csv('data/Embedding_grarep'+str(i)+'.txt', sep=' ',header=None, error_bad_lines=False)\n",
    "    Embedding_grarep = Embedding_grarep.sort_values(0,ascending=True)# 建立序号排序\n",
    "    Embedding_grarep.set_index(0, inplace=True)\n",
    "    Embedding_grarep['index'] = Embedding_grarep.index\n",
    "    Embedding = pd.merge(dd,Embedding_grarep,how='left',on='index')\n",
    "    Embedding = Embedding.fillna(int(0)).iloc[:,2:]\n",
    "    train_data = pd.read_csv('data/DTIs_train'+str(i)+'.csv',header=None)\n",
    "    train_data[2] = train_data.apply(lambda x: 1 if x[0] < 0 else 1, axis=1)\n",
    "    kk = []\n",
    "    for j in range(5):\n",
    "        if j !=i:\n",
    "            kk.append(j)\n",
    "    index = np.hstack([np.array(Nindex)[kk[0]],np.array(Nindex)[kk[1]],np.array(Nindex)[kk[2]],np.array(Nindex)[kk[3]]])\n",
    "    result = train_data.append(pd.DataFrame(np.array(Negative)[index]))    \n",
    "    labels_train = result[2]# np.hstack([np.array(Embedding_GCN)[result.iloc[r,0]],np.array(Embedding_Node2vec)[result.iloc[r,0]]])\n",
    "    for r in range(len(result)):# np.hstack([np.array(Embedding_GCN)[result.iloc[r,1]],np.array(Embedding_Node2vec)[result.iloc[r,1]]])\n",
    "        data_train_feature.append(np.hstack([np.hstack([np.array(Embedding_GCN)[result.iloc[r,0]],np.array(Embedding)[result.iloc[r,0]]]),\n",
    "                                             np.hstack([np.array(Embedding_GCN)[result.iloc[r,1]],np.array(Embedding)[result.iloc[r,1]]])]))\n",
    "    creat_var['data_train'+str(i)] = data_train_feature\n",
    "    creat_var['labels_train'+str(i)] = labels_train\n",
    "    print(len(labels_train))\n",
    "    del labels_train, result, data_train_feature, r\n",
    "    test_data = pd.read_csv('data/DTIs_test'+str(i)+'.csv',header=None)\n",
    "    test_data[2] = test_data.apply(lambda x: 1 if x[0] < 0 else 1, axis=1)\n",
    "    result = test_data.append(pd.DataFrame(np.array(Negative)[np.array(Nindex)[i]]))    \n",
    "    labels_test = result[2]# np.hstack([np.array(Embedding_GCN)[result.iloc[x,0]],np.array(Embedding_Node2vec)[result.iloc[x,0]]])\n",
    "    for x in range(len(result)):#np.hstack([np.array(Embedding_GCN)[result.iloc[x,1]],np.array(Embedding_Node2vec)[result.iloc[x,1]]])\n",
    "        data_test_feature.append(np.hstack([np.hstack([np.array(Embedding_GCN)[result.iloc[x,0]],np.array(Embedding)[result.iloc[x,0]]]),\n",
    "                                            np.hstack([np.array(Embedding_GCN)[result.iloc[x,1]],np.array(Embedding)[result.iloc[x,1]]])]))\n",
    "    creat_var['data_test'+str(i)] = data_test_feature\n",
    "    creat_var['labels_test'+str(i)] = labels_test\n",
    "    print(len(labels_test))\n",
    "    del train_data, test_data, labels_test, result, data_test_feature, x#, Embedding_Node2vec, Embedding, Embedding_GCN    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [data_train0,data_train1,data_train2,data_train3,data_train4]\n",
    "data_test = [data_test0,data_test1,data_test2,data_test3,data_test4]\n",
    "labels_train = [labels_train0,labels_train1,labels_train2,labels_train3,labels_train4]\n",
    "labels_test = [labels_test0,labels_test1,labels_test2,labels_test3,labels_test4]\n",
    "print(np.array(data_train0).shape)\n",
    "print(np.array(data_test0).shape)\n",
    "print(np.array(labels_train0).shape)\n",
    "print(np.array(labels_test0).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评价指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评价指标\n",
    "import math\n",
    "def MyConfusionMatrix(y_real,y_predict): \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    CM = confusion_matrix(y_real, y_predict)\n",
    "    print(CM)\n",
    "    CM = CM.tolist()\n",
    "    TN = CM[0][0]\n",
    "    FP = CM[0][1]\n",
    "    FN = CM[1][0]\n",
    "    TP = CM[1][1] \n",
    "    print('TN:%d, FP:%d, FN:%d, TP:%d' % (TN, FP, FN, TP))\n",
    "    Acc = (TN + TP) / (TN + TP + FN + FP)\n",
    "    Sen = TP / (TP + FN)\n",
    "    Spec = TN / (TN + FP)\n",
    "    Prec = TP / (TP + FP)\n",
    "    MCC = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "    # 分母可能出现0，需要讨论待续\n",
    "    print('Acc:', round(Acc, 4))\n",
    "    print('Sen:', round(Sen, 4))\n",
    "    print('Spec:', round(Spec, 4))\n",
    "    print('Prec:', round(Prec, 4))\n",
    "    print('MCC:', round(MCC, 4))\n",
    "    Result = []\n",
    "    Result.append(round(Acc, 4))\n",
    "    Result.append(round(Sen, 4))\n",
    "    Result.append(round(Spec, 4))\n",
    "    Result.append(round(Prec, 4))\n",
    "    Result.append(round(MCC, 4))\n",
    "    return Result\n",
    "\n",
    "def MyAverage(matrix):\n",
    "    SumAcc = 0\n",
    "    SumSen = 0\n",
    "    SumSpec = 0\n",
    "    SumPrec = 0\n",
    "    SumMcc = 0\n",
    "    counter = 0\n",
    "    while counter < len(matrix):\n",
    "        SumAcc = SumAcc + matrix[counter][0]\n",
    "        SumSen = SumSen + matrix[counter][1]\n",
    "        SumSpec = SumSpec + matrix[counter][2]\n",
    "        SumPrec = SumPrec + matrix[counter][3]\n",
    "        SumMcc = SumMcc + matrix[counter][4]\n",
    "        counter = counter + 1\n",
    "    print('AverageAcc:',SumAcc / len(matrix))\n",
    "    print('AverageSen:', SumSen / len(matrix))\n",
    "    print('AverageSpec:', SumSpec / len(matrix))\n",
    "    print('AveragePrec:', SumPrec / len(matrix))\n",
    "    print('AverageMcc:', SumMcc / len(matrix))\n",
    "    return\n",
    "\n",
    "def MyStd(result):\n",
    "    import numpy as np\n",
    "    NewMatrix = []\n",
    "    counter = 0\n",
    "    while counter < len(result[0]):\n",
    "        row = []\n",
    "        NewMatrix.append(row)\n",
    "        counter = counter + 1\n",
    "    counter = 0\n",
    "    while counter < len(result):\n",
    "        counter1 = 0\n",
    "        while counter1 < len(result[counter]):\n",
    "            NewMatrix[counter1].append(result[counter][counter1])\n",
    "            counter1 = counter1 + 1\n",
    "        counter = counter + 1\n",
    "    StdList = []\n",
    "    MeanList = []\n",
    "    counter = 0\n",
    "    while counter < len(NewMatrix):\n",
    "        # std\n",
    "        arr_std = np.std(NewMatrix[counter], ddof=1)\n",
    "        StdList.append(arr_std)\n",
    "        # mean\n",
    "        arr_mean = np.mean(NewMatrix[counter])\n",
    "        MeanList.append(arr_mean)\n",
    "        counter = counter + 1\n",
    "    result.append(MeanList)\n",
    "    result.append(StdList)\n",
    "    # 换算成百分比制\n",
    "    counter = 0\n",
    "    while counter < len(result):\n",
    "        counter1 = 0\n",
    "        while counter1 < len(result[counter]):\n",
    "            result[counter][counter1] = round(result[counter][counter1] * 100, 2)\n",
    "            counter1 = counter1 + 1\n",
    "        counter = counter + 1\n",
    "    return result\n",
    "\n",
    "import csv\n",
    "def StorFile(data, fileName):\n",
    "    with open(fileName, \"w\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerows(data)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 画ROC曲线，计算AUC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy import interp\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time()))\n",
    "\n",
    "# print(\"迭代寻找最优参数\")\n",
    "# k_range = [301, 401, 501, 601, 701, 801, 901, 999]\n",
    "# cv_scores = [] #用来放每个模型的结果值\n",
    "# for n in k_range:\n",
    "#     print('n_estimators: %d '%(n))\n",
    "#     RandomF = RandomForestClassifier(n_estimators=n)   #knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV\n",
    "#     scores = cross_val_score(RandomF,data_train0, labels_train0,\n",
    "#                              cv=5,\n",
    "#                              scoring='roc_auc', \n",
    "#                              n_jobs=-1)  #cv：选择每次测试折数  accuracy：评价指标是准确度,可以省略使用默认值，具体使用参考下面。\n",
    "#     cv_scores.append(scores.mean())\n",
    "# print(\"best_n_neighbors is：\", k_range[cv_scores.index(max(cv_scores))])\n",
    "\n",
    "print(\"进行5折交叉验证\")\n",
    "tprs=[]\n",
    "aucs=[]\n",
    "mean_fpr=np.linspace(0,1,1000)\n",
    "# 定义i用于绘制每次训练的ROC曲线\n",
    "\n",
    "# ROC曲线颜色\n",
    "colorlist = ['red', 'gold', 'purple', 'green', 'blue', 'black']\n",
    "# 保存评价指标\n",
    "AllResult = []\n",
    "\n",
    "#data为数据集,利用KF.split划分训练集和测试集\n",
    "for i in range(5):\n",
    "    #建立模型，并对训练集进行测试，求出预测得分\n",
    "    #划分训练集和测试集\n",
    "    print('正在划分训练集和测试集...')\n",
    "    X_train,X_test = data_train[i],data_test[i]\n",
    "    Y_train,Y_test = np.array(labels_train[i]),np.array(labels_test[i])\n",
    "    print('划分完毕！')\n",
    "    #建立模型 # n_estimators=k_range[cv_scores.index(max(cv_scores))]\n",
    "    best_XGB = XGBClassifier(learning_rate=0.15,max_depth=500,n_estimators=500)\n",
    "#                                  n_estimators=100,  ##树的个数\n",
    "#                                  max_depth=4,        ##树额深度\n",
    "#                                  min_child_weight=1,  ##叶子节点最小权重\n",
    "#                                  gamma=0,             ##惩罚项中叶子节点个数前的参数\n",
    "#                                  subsample=1,         ##所有样本建立决策树\n",
    "#                                  colsample_btree=1,   ##所有特征建立决策树\n",
    "#                                  scale_pos_weight=1,  ##解决样本个数不平衡的问题\n",
    "#                                  random_state=27,     ##随机数\n",
    "#                                  slient=0)\n",
    "    print('开始训练')\n",
    "    #训练模型\n",
    "    best_XGB.fit(np.array(X_train), np.array(Y_train))\n",
    "    # 保存模型\n",
    "    joblib.dump(best_XGB,'model/'+'XGB'+str(i)+'.pkl')\n",
    "    #利用model.predict获取测试集的预测值\n",
    "    y_score0 = best_XGB.predict(np.array(X_test))\n",
    "    y_score_XGB = best_XGB.predict_proba(np.array(X_test))\n",
    "    # 输出混淆矩阵\n",
    "    print(confusion_matrix(Y_test, y_score0)) # , labels=[1,0]\n",
    "    \n",
    "    dd = np.vstack([Y_test, y_score_XGB[:,1]]).T\n",
    "    XGB_data = pd.DataFrame(dd)\n",
    "    XGB_data.to_csv('predict/' + 'XGB_'+ str(i)+ 'Prob.csv', header = False, index = False)\n",
    "    \n",
    "    #计算fpr(假阳性率),tpr(真阳性率),thresholds(阈值)[绘制ROC曲线要用到这几个值]\n",
    "    fpr,tpr,thresholds=roc_curve(Y_test,y_score_XGB[:,1])\n",
    "    #interp:插值 把结果添加到tprs列表中 \n",
    "    tprs.append(interp(mean_fpr,fpr,tpr))\n",
    "    tprs[-1][0]=0.0\n",
    "    #计算auc\n",
    "    roc_auc=auc(fpr,tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数计算出来\n",
    "    plt.plot(fpr,tpr,lw=1.5,alpha=0.8,color=colorlist[i],\n",
    "             label='ROC fold %d(AUC=%0.4f)'% (i,roc_auc))\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    print(\"fold = \", i)\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    Result = MyConfusionMatrix(Y_test, y_score0)\n",
    "    AllResult.append(Result)\n",
    "    AllResult[i].append(roc_auc)\n",
    "    i +=1\n",
    "\n",
    "# 保存各种评价指标\n",
    "MyAverage(AllResult)\n",
    "# AllResult\n",
    "# print('AllResult', AllResult)\n",
    "MyNew = MyStd(AllResult)\n",
    "# StorFile(MyNew, '五折的评价指标.csv')\n",
    "df = pd.DataFrame(data = MyNew)\n",
    "df.to_csv('evaluate/XGB_5-fold.csv', encoding='utf-8',header=None,index=False)\n",
    "\n",
    "#画对角线\n",
    "plt.plot([0,1],[0,1],linestyle='--',lw=2,color='rosybrown',alpha=0.8)\n",
    "mean_tpr=np.mean(tprs,axis=0)\n",
    "mean_tpr[-1]=1.0\n",
    "mean_auc=auc(mean_fpr,mean_tpr)#计算平均AUC值\n",
    "std_auc=np.std(tprs,axis=0)\n",
    "plt.plot(mean_fpr,mean_tpr,color=colorlist[i],label=r'Mean ROC (AUC=%0.4f)'%mean_auc,lw=2,alpha=1)\n",
    "std_tpr=np.std(tprs,axis=0)\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('image/'+ now + 'XGB.svg')\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 画5折的PR图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "import numpy as np\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 1000)\n",
    "i = 0\n",
    "colorlist = ['red', 'gold', 'purple', 'green', 'blue', 'black']\n",
    "\n",
    "Ps = []\n",
    "#Rs = []\n",
    "RPs = []\n",
    "mean_R = np.linspace(0, 1, 1000)\n",
    "#Reals = []\n",
    "#PredictionProbs = []\n",
    "\n",
    "for i in range(5):\n",
    "    # 读取文件\n",
    "    RAPNameProb = 'predict/' + 'XGB_'+ str(i)+ 'Prob.csv'\n",
    "    RealAndPredictionProb = pd.read_csv(RAPNameProb, header=None)\n",
    "    \n",
    "    Real = RealAndPredictionProb[0]\n",
    "    # Prediction = RealAndPrediction[1]\n",
    "    PredictionProb = RealAndPredictionProb[1]\n",
    "\n",
    "    average_precision = average_precision_score(Real, PredictionProb)\n",
    "    precision, recall, _ = precision_recall_curve(Real, PredictionProb)\n",
    "\n",
    "    Ps.append(interp(mean_R, precision, recall))\n",
    "    RPs.append(average_precision)\n",
    "\n",
    "    #Reals.append(Real)\n",
    "    #PredictionProbs.append(PredictionProb)\n",
    "    # 弧线\n",
    "    plt.plot(recall, precision, lw=1.5, alpha=0.8, color=colorlist[i],\n",
    "             label='fold %d (AUPR = %0.4f)' % (i, average_precision))\n",
    "    print('average_precision', average_precision)\n",
    "\n",
    "#  画均值\n",
    "mean_P = np.mean(Ps, axis=0)\n",
    "mean_RPs = np.mean(RPs, axis=0)\n",
    "#std_RPs = np.std(RPs)\n",
    "plt.plot(mean_P, mean_R, color='black',\n",
    "         label=r'Mean (AUPR = %0.4f)' % (mean_RPs),\n",
    "         lw=2, alpha=1)\n",
    "plt.xlabel('Recall',fontsize=13)\n",
    "plt.ylabel('Precision',fontsize=13)\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "# 画对角线\n",
    "plt.plot([1, 0], [0, 1], color='rosybrown', lw=2, linestyle='--')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('image/XGB_PR-5fold.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 比较不同的分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DecisionTree的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 画ROC曲线，计算AUC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy import interp\n",
    "import time\n",
    "\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time()))\n",
    "\n",
    "# print(\"迭代寻找最优参数\")\n",
    "# k_range = [301, 401, 501, 601, 701, 801, 901, 999]\n",
    "# cv_scores = [] #用来放每个模型的结果值\n",
    "# for n in k_range:\n",
    "#     print('n_estimators: %d '%(n))\n",
    "#     RandomF = RandomForestClassifier(n_estimators=n)   #knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV\n",
    "#     scores = cross_val_score(RandomF,data_train0, labels_train0,\n",
    "#                              cv=5,\n",
    "#                              scoring='roc_auc', \n",
    "#                              n_jobs=-1)  #cv：选择每次测试折数  accuracy：评价指标是准确度,可以省略使用默认值，具体使用参考下面。\n",
    "#     cv_scores.append(scores.mean())\n",
    "# print(\"best_n_neighbors is：\", k_range[cv_scores.index(max(cv_scores))])\n",
    "\n",
    "print(\"进行5折交叉验证\")\n",
    "tprs=[]\n",
    "aucs=[]\n",
    "mean_fpr=np.linspace(0,1,1000)\n",
    "# 定义i用于绘制每次训练的ROC曲线\n",
    "\n",
    "# ROC曲线颜色\n",
    "colorlist = ['red', 'gold', 'purple', 'green', 'blue', 'black']\n",
    "# 保存评价指标\n",
    "AllResult = []\n",
    "\n",
    "#data为数据集,利用KF.split划分训练集和测试集\n",
    "for i in range(5):\n",
    "    #建立模型，并对训练集进行测试，求出预测得分\n",
    "    #划分训练集和测试集\n",
    "    print('正在划分训练集和测试集...')\n",
    "    X_train,X_test = data_train[i],data_test[i]\n",
    "    Y_train,Y_test = np.array(labels_train[i]),np.array(labels_test[i])\n",
    "    print('划分完毕！')\n",
    "#     best_adb = HistGradientBoostingClassifier()\n",
    "    #建立模型 # n_estimators=k_range[cv_scores.index(max(cv_scores))]\n",
    "    best_cart = DecisionTreeClassifier()\n",
    "    print('开始训练')\n",
    "    #训练模型\n",
    "    best_cart.fit(np.array(X_train), np.array(Y_train))\n",
    "    # 保存模型\n",
    "    joblib.dump(best_cart,'model/'+'CART'+str(i)+'.pkl')\n",
    "    #利用model.predict获取测试集的预测值\n",
    "    y_score0 = best_cart.predict(np.array(X_test))\n",
    "    y_score_cart = best_cart.predict_proba(np.array(X_test))\n",
    "    # 输出混淆矩阵\n",
    "    print(confusion_matrix(Y_test, y_score0)) # , labels=[1,0]\n",
    "    \n",
    "    dd = np.vstack([Y_test, y_score_cart[:,1]]).T\n",
    "    cart_data = pd.DataFrame(dd)\n",
    "    cart_data.to_csv('predict/' + 'CART_'+ str(i)+ 'Prob.csv', header = False, index = False)\n",
    "    \n",
    "    #计算fpr(假阳性率),tpr(真阳性率),thresholds(阈值)[绘制ROC曲线要用到这几个值]\n",
    "    fpr,tpr,thresholds=roc_curve(Y_test,y_score_cart[:,1])\n",
    "    #interp:插值 把结果添加到tprs列表中 \n",
    "    tprs.append(interp(mean_fpr,fpr,tpr))\n",
    "    tprs[-1][0]=0.0\n",
    "    #计算auc\n",
    "    roc_auc=auc(fpr,tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数计算出来\n",
    "    plt.plot(fpr,tpr,lw=1.5,alpha=0.8,color=colorlist[i],\n",
    "             label='ROC fold %d(AUC=%0.4f)'% (i,roc_auc))\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    print(\"fold = \", i)\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    Result = MyConfusionMatrix(Y_test, y_score0)\n",
    "    AllResult.append(Result)\n",
    "    AllResult[i].append(roc_auc)\n",
    "    i +=1\n",
    "\n",
    "# 保存各种评价指标\n",
    "MyAverage(AllResult)\n",
    "# AllResult\n",
    "# print('AllResult', AllResult)\n",
    "MyNew = MyStd(AllResult)\n",
    "# StorFile(MyNew, '五折的评价指标.csv')\n",
    "df = pd.DataFrame(data = MyNew)\n",
    "df.to_csv('evaluate/CART_5-fold.csv', encoding='utf-8',header=None,index=False)\n",
    "\n",
    "#画对角线\n",
    "plt.plot([0,1],[0,1],linestyle='--',lw=2,color='rosybrown',alpha=0.8)\n",
    "mean_tpr=np.mean(tprs,axis=0)\n",
    "mean_tpr[-1]=1.0\n",
    "mean_auc=auc(mean_fpr,mean_tpr)#计算平均AUC值\n",
    "std_auc=np.std(tprs,axis=0)\n",
    "plt.plot(mean_fpr,mean_tpr,color=colorlist[i],label=r'Mean ROC (AUC=%0.4f)'%mean_auc,lw=2,alpha=1)\n",
    "std_tpr=np.std(tprs,axis=0)\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('image/'+ now + 'CART_ROC.svg')\n",
    "plt.show()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GaussianNB的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 画ROC曲线，计算AUC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy import interp\n",
    "import time\n",
    "\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time()))\n",
    "\n",
    "# print(\"迭代寻找最优参数\")\n",
    "# k_range = range(500,1000,20)\n",
    "# cv_scores = []#用来放每个模型的结果值\n",
    "# for n in k_range:\n",
    "#     GBDT = GradientBoostingClassifier(n_estimators=n, subsample=0.8)   #knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV\n",
    "#     scores = cross_val_score(GBDT,np.hstack([np.array(X_train1), np.array(X_train2)]), np.array(y_train1),cv=5,scoring='accuracy',n_jobs=-1)  #cv：选择每次测试折数  accuracy：评价指标是准确度,可以省略使用默认值，具体使用参考下面。\n",
    "#     cv_scores.append(scores.mean())\n",
    "# print(\"best_n_neighbors is：\"k_range[cv_scores.index(max(cv_scores))])\n",
    "\n",
    "print(\"进行5折交叉验证\")\n",
    "tprs=[]\n",
    "aucs=[]\n",
    "mean_fpr=np.linspace(0,1,1000)\n",
    "# 定义i用于绘制每次训练的ROC曲线\n",
    "\n",
    "# ROC曲线颜色\n",
    "colorlist = ['red', 'gold', 'purple', 'green', 'blue', 'black']\n",
    "# 保存评价指标\n",
    "AllResult = []\n",
    "\n",
    "#data为数据集,利用KF.split划分训练集和测试集\n",
    "for i in range(5):\n",
    "    #建立模型，并对训练集进行测试，求出预测得分\n",
    "    #划分训练集和测试集\n",
    "    print('正在划分训练集和测试集...')\n",
    "    X_train,X_test = data_train[i],data_test[i]\n",
    "    Y_train,Y_test = np.array(labels_train[i]),np.array(labels_test[i])\n",
    "    print('划分完毕！')\n",
    "    #建立模型 # n_estimators=k_range[cv_scores.index(max(cv_scores))]\n",
    "    best_nb = GaussianNB()# 选择最优的K=3传入模型\n",
    "    print('开始训练')\n",
    "    #训练模型\n",
    "    best_nb.fit(np.array(X_train), np.array(Y_train))\n",
    "    # 保存模型\n",
    "    joblib.dump(best_nb,'model/'+'GNB'+str(i)+'.pkl')\n",
    "    #利用model.predict获取测试集的预测值\n",
    "    y_score0 = best_nb.predict(np.array(X_test))\n",
    "    y_score_nb = best_nb.predict_proba(np.array(X_test))\n",
    "    # 输出混淆矩阵\n",
    "    print(confusion_matrix(Y_test, y_score0))\n",
    "    \n",
    "    dd = np.vstack([Y_test, y_score_nb[:,1]]).T\n",
    "    nb_data = pd.DataFrame(dd)\n",
    "    nb_data.to_csv('predict/' + 'GNB_'+ str(i)+ 'Prob.csv', header = False, index = False)\n",
    "    \n",
    "    #计算fpr(假阳性率),tpr(真阳性率),thresholds(阈值)[绘制ROC曲线要用到这几个值]\n",
    "    fpr,tpr,thresholds=roc_curve(Y_test,y_score_nb[:,1])\n",
    "    #interp:插值 把结果添加到tprs列表中 \n",
    "    tprs.append(interp(mean_fpr,fpr,tpr))\n",
    "    tprs[-1][0]=0.0\n",
    "    #计算auc\n",
    "    roc_auc=auc(fpr,tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数计算出来\n",
    "    plt.plot(fpr,tpr,lw=1.5,alpha=0.8,color=colorlist[i],\n",
    "             label='ROC fold %d(AUC=%0.4f)'% (i,roc_auc))\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    print(\"fold = \", i)\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    Result = MyConfusionMatrix(Y_test, y_score0)\n",
    "    AllResult.append(Result)\n",
    "    AllResult[i].append(roc_auc)\n",
    "    i +=1\n",
    "\n",
    "# 保存各种评价指标\n",
    "MyAverage(AllResult)\n",
    "# AllResult\n",
    "# print('AllResult', AllResult)\n",
    "MyNew = MyStd(AllResult)\n",
    "# StorFile(MyNew, '五折的评价指标.csv')\n",
    "df = pd.DataFrame(data = MyNew)\n",
    "df.to_csv('evaluate/GNB_5-fold.csv', encoding='utf-8',header=None,index=False)\n",
    "\n",
    "#画对角线\n",
    "plt.plot([0,1],[0,1],linestyle='--',lw=2,color='rosybrown',alpha=0.8)\n",
    "mean_tpr=np.mean(tprs,axis=0)\n",
    "mean_tpr[-1]=1.0\n",
    "mean_auc=auc(mean_fpr,mean_tpr)#计算平均AUC值\n",
    "std_auc=np.std(tprs,axis=0)\n",
    "plt.plot(mean_fpr,mean_tpr,color=colorlist[i],label=r'Mean ROC (AUC=%0.4f)'%mean_auc,lw=2,alpha=1)\n",
    "std_tpr=np.std(tprs,axis=0)\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('image/'+ now + 'GNB_ROC.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import joblib\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 画ROC曲线，计算AUC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy import interp\n",
    "import time\n",
    "\n",
    "now = time.strftime(\"%Y-%m-%d-%H_%M_%S\",time.localtime(time.time()))\n",
    "\n",
    "# print(\"迭代寻找最优参数\")\n",
    "# k_range = range(500,1000,20)\n",
    "# cv_scores = []#用来放每个模型的结果值\n",
    "# for n in k_range:\n",
    "#     GBDT = GradientBoostingClassifier(n_estimators=n, subsample=0.8)   #knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV\n",
    "#     scores = cross_val_score(GBDT,np.hstack([np.array(X_train1), np.array(X_train2)]), np.array(y_train1),cv=5,scoring='accuracy',n_jobs=-1)  #cv：选择每次测试折数  accuracy：评价指标是准确度,可以省略使用默认值，具体使用参考下面。\n",
    "#     cv_scores.append(scores.mean())\n",
    "# print(\"best_n_neighbors is：\"k_range[cv_scores.index(max(cv_scores))])\n",
    "\n",
    "print(\"进行5折交叉验证\")\n",
    "tprs=[]\n",
    "aucs=[]\n",
    "mean_fpr=np.linspace(0,1,1000)\n",
    "# 定义i用于绘制每次训练的ROC曲线\n",
    "\n",
    "# ROC曲线颜色\n",
    "colorlist = ['red', 'gold', 'purple', 'green', 'blue', 'black']\n",
    "# 保存评价指标\n",
    "AllResult = []\n",
    "\n",
    "#data为数据集,利用KF.split划分训练集和测试集\n",
    "for i in range(5):\n",
    "    #建立模型，并对训练集进行测试，求出预测得分\n",
    "    #划分训练集和测试集\n",
    "    print('正在划分训练集和测试集...')\n",
    "    X_train,X_test = data_train[i],data_test[i]\n",
    "    Y_train,Y_test = np.array(labels_train[i]),np.array(labels_test[i])\n",
    "    print('划分完毕！')\n",
    "    #建立模型 # n_estimators=k_range[cv_scores.index(max(cv_scores))]\n",
    "    best_SVC = SVC(probability=True)# 选择最优的K=3传入模型\n",
    "    print('开始训练')\n",
    "    #训练模型\n",
    "    best_SVC.fit(np.array(X_train), np.array(Y_train))\n",
    "    # 保存模型\n",
    "    joblib.dump(best_SVC,'model/'+'SVC'+str(i)+'.pkl')\n",
    "    #利用model.predict获取测试集的预测值\n",
    "    y_score0 = best_SVC.predict(np.array(X_test))\n",
    "    y_score_SVC = best_SVC.predict_proba(np.array(X_test))\n",
    "    # 输出混淆矩阵\n",
    "    print(confusion_matrix(Y_test, y_score0))\n",
    "    \n",
    "    dd = np.vstack([Y_test, y_score_SVC[:,1]]).T\n",
    "    SVC_data = pd.DataFrame(dd)\n",
    "    SVC_data.to_csv('predict/' + 'SVC_'+ str(i)+ 'Prob.csv', header = False, index = False)\n",
    "    \n",
    "    #计算fpr(假阳性率),tpr(真阳性率),thresholds(阈值)[绘制ROC曲线要用到这几个值]\n",
    "    fpr,tpr,thresholds=roc_curve(Y_test,y_score_SVC[:,1])\n",
    "    #interp:插值 把结果添加到tprs列表中 \n",
    "    tprs.append(interp(mean_fpr,fpr,tpr))\n",
    "    tprs[-1][0]=0.0\n",
    "    #计算auc\n",
    "    roc_auc=auc(fpr,tpr)\n",
    "    aucs.append(roc_auc)\n",
    "    #画图，只需要plt.plot(fpr,tpr),变量roc_auc只是记录auc的值，通过auc()函数计算出来\n",
    "    plt.plot(fpr,tpr,lw=1.5,alpha=0.8,color=colorlist[i],\n",
    "             label='ROC fold %d(AUC=%0.4f)'% (i,roc_auc))\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    print(\"fold = \", i)\n",
    "    print(\"---------------------------------------------\\n\")\n",
    "    Result = MyConfusionMatrix(Y_test, y_score0)\n",
    "    AllResult.append(Result)\n",
    "    AllResult[i].append(roc_auc)\n",
    "    i +=1\n",
    "\n",
    "# 保存各种评价指标\n",
    "MyAverage(AllResult)\n",
    "# AllResult\n",
    "# print('AllResult', AllResult)\n",
    "MyNew = MyStd(AllResult)\n",
    "# StorFile(MyNew, '五折的评价指标.csv')\n",
    "df = pd.DataFrame(data = MyNew)\n",
    "df.to_csv('evaluate/SVC_5-fold.csv', encoding='utf-8',header=None,index=False)\n",
    "\n",
    "\n",
    "#画对角线\n",
    "plt.plot([0,1],[0,1],linestyle='--',lw=2,color='rosybrown',alpha=0.8)\n",
    "mean_tpr=np.mean(tprs,axis=0)\n",
    "mean_tpr[-1]=1.0\n",
    "mean_auc=auc(mean_fpr,mean_tpr)#计算平均AUC值\n",
    "std_auc=np.std(tprs,axis=0)\n",
    "plt.plot(mean_fpr,mean_tpr,color=colorlist[i],label=r'Mean ROC (AUC=%0.4f)'%mean_auc,lw=2,alpha=1)\n",
    "std_tpr=np.std(tprs,axis=0)\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('image/'+ now + 'SVC_ROC.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 比较不同分类器画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 画ROC曲线，计算AUC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy import interp\n",
    "import time\n",
    "\n",
    "colorlist = ['red', 'gold', 'purple', 'limegreen', 'darkblue', 'k']\n",
    "\n",
    "for i in range(1):\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 1000)\n",
    "    for j in range(5):\n",
    "        # 读取文件\n",
    "        RAPNameProb = 'predict/XGB_' + str(j) + 'Prob.csv'\n",
    "        RealAndPredictionProb = pd.read_csv(RAPNameProb, header=None)\n",
    "\n",
    "        Real = RealAndPredictionProb[0]\n",
    "        PredictionProb = RealAndPredictionProb[1]\n",
    "\n",
    "        # 画图\n",
    "        fpr, tpr, thresholds = roc_curve(Real,PredictionProb)\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "    \n",
    "     # 画均值\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr) \n",
    "    mean_tpr=np.mean(tprs,axis=0)\n",
    "    mean_tpr[-1]=1.0\n",
    "    mean_auc=auc(mean_fpr,mean_tpr)#计算平均AUC值\n",
    "    plt.plot(mean_fpr,mean_tpr,color=colorlist[5],label=r'XGB(AUC=%0.4f)'%mean_auc,lw=1.5,alpha=1)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 1000)\n",
    "    for j in range(5):\n",
    "        # 读取文件\n",
    "        RAPNameProb = 'predict/CART_' + str(j) + 'Prob.csv'  \n",
    "        RealAndPredictionProb = pd.read_csv(RAPNameProb, header=None)\n",
    "\n",
    "        Real = RealAndPredictionProb[0]\n",
    "        PredictionProb = RealAndPredictionProb[1]\n",
    "\n",
    "        # 画图\n",
    "        fpr, tpr, thresholds = roc_curve(Real,PredictionProb)\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "\n",
    "     # 画均值\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr) \n",
    "    mean_tpr=np.mean(tprs,axis=0)\n",
    "    mean_tpr[-1]=1.0\n",
    "    mean_auc=auc(mean_fpr,mean_tpr)#计算平均AUC值\n",
    "    plt.plot(mean_fpr,mean_tpr,linestyle='-',color=colorlist[3],label=r'CART(AUC=%0.4f)'%mean_auc,lw=1.5,alpha=1)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 1000)\n",
    "    for j in range(5):\n",
    "        # 读取文件\n",
    "        RAPNameProb = 'predict/GNB_' + str(j) + 'Prob.csv'\n",
    "        RealAndPredictionProb = pd.read_csv(RAPNameProb, header=None)\n",
    "\n",
    "        Real = RealAndPredictionProb[0]\n",
    "        PredictionProb = RealAndPredictionProb[1]\n",
    "\n",
    "        # 画图\n",
    "        fpr, tpr, thresholds = roc_curve(Real,PredictionProb)\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "\n",
    "     # 画均值\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr) \n",
    "    mean_tpr=np.mean(tprs,axis=0)\n",
    "    mean_tpr[-1]=1.0\n",
    "    mean_auc=auc(mean_fpr,mean_tpr)#计算平均AUC值\n",
    "    plt.plot(mean_fpr,mean_tpr,linestyle='--',color=colorlist[0],label=r'GNB(AUC=%0.4f)'%mean_auc,lw=1.5,alpha=1)\n",
    "    \n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 1000)\n",
    "    for j in range(5):\n",
    "        # 读取文件\n",
    "        RAPNameProb = 'predict/SVC_' + str(j) + 'Prob.csv'\n",
    "        RealAndPredictionProb = pd.read_csv(RAPNameProb, header=None)\n",
    "\n",
    "        Real = RealAndPredictionProb[0]\n",
    "        PredictionProb = RealAndPredictionProb[1]\n",
    "\n",
    "        # 画图\n",
    "        fpr, tpr, thresholds = roc_curve(Real,PredictionProb)\n",
    "        tprs.append(interp(mean_fpr, fpr, tpr))\n",
    "        tprs[-1][0] = 0.0\n",
    "\n",
    "     # 画均值\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr) \n",
    "    mean_tpr=np.mean(tprs,axis=0)\n",
    "    mean_tpr[-1]=1.0\n",
    "    mean_auc=auc(mean_fpr,mean_tpr)#计算平均AUC值\n",
    "    plt.plot(mean_fpr,mean_tpr,linestyle='-.',color=colorlist[4],label=r'SVM(AUC=%0.4f)'%mean_auc,lw=1.5,alpha=1)  \n",
    "      \n",
    "plt.plot([0,1],[0,1],linestyle='--',lw=2,color='rosybrown',alpha=0.8)    \n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='best')   \n",
    "plt.savefig('image/Compare_ROC.svg')\n",
    "plt.savefig('image/Compare_ROC.tif')\n",
    "plt.show()      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画PR曲线\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "import numpy as np\n",
    "\n",
    "colorlist = ['red', 'gold', 'purple', 'limegreen', 'darkblue', 'k']\n",
    "\n",
    "for i in range(1): \n",
    "    Ps = []\n",
    "    RPs = []\n",
    "    mean_R = np.linspace(0, 1, 1000)\n",
    "    for j in range(5):\n",
    "        # 读取文件\n",
    "        RAPNameProb = 'predict/XGB_' + str(j) + 'Prob.csv'\n",
    "        RealAndPredictionProb = pd.read_csv(RAPNameProb, header=None)\n",
    "\n",
    "        Real = RealAndPredictionProb[0]\n",
    "        PredictionProb = RealAndPredictionProb[1]\n",
    "\n",
    "        # 画图\n",
    "        average_precision = average_precision_score(Real, PredictionProb)\n",
    "        precision, recall, _ = precision_recall_curve(Real, PredictionProb)\n",
    "\n",
    "        Ps.append(interp(mean_R, precision, recall))\n",
    "        RPs.append(average_precision)\n",
    "    \n",
    "    # 画均值\n",
    "    mean_P = np.mean(Ps, axis=0)\n",
    "    mean_RPs = np.mean(RPs, axis=0)\n",
    "    plt.plot(mean_P, mean_R,color=colorlist[5],label=r'XGB(AUPR=%0.4f)'%mean_RPs,lw=1.5,alpha=1)\n",
    "    Ps = []\n",
    "    RPs = []\n",
    "    mean_R = np.linspace(0, 1, 1000)\n",
    "    for j in range(5):\n",
    "        # 读取文件\n",
    "        RAPNameProb = 'predict/CART_' + str(j) + 'Prob.csv'\n",
    "        RealAndPredictionProb = pd.read_csv(RAPNameProb, header=None)\n",
    "\n",
    "        Real = RealAndPredictionProb[0]\n",
    "        PredictionProb = RealAndPredictionProb[1]\n",
    "\n",
    "        # 画图\n",
    "        average_precision = average_precision_score(Real, PredictionProb)\n",
    "        precision, recall, _ = precision_recall_curve(Real, PredictionProb)\n",
    "\n",
    "        Ps.append(interp(mean_R, precision, recall))\n",
    "        RPs.append(average_precision)\n",
    "\n",
    "     # 画均值\n",
    "    mean_P = np.mean(Ps, axis=0)\n",
    "    mean_RPs = np.mean(RPs, axis=0)\n",
    "    plt.plot(mean_P, mean_R,linestyle='-',color=colorlist[3],label=r'CART(AUPR=%0.4f)'%mean_RPs,lw=1.5,alpha=1)\n",
    "    Ps = []\n",
    "    RPs = []\n",
    "    mean_R = np.linspace(0, 1, 1000)\n",
    "    for j in range(5):\n",
    "        # 读取文件\n",
    "        RAPNameProb = 'predict/GNB_' + str(j) + 'Prob.csv'\n",
    "        RealAndPredictionProb = pd.read_csv(RAPNameProb, header=None)\n",
    "\n",
    "        Real = RealAndPredictionProb[0]\n",
    "        PredictionProb = RealAndPredictionProb[1]\n",
    "\n",
    "        # 画图\n",
    "        average_precision = average_precision_score(Real, PredictionProb)\n",
    "        precision, recall, _ = precision_recall_curve(Real, PredictionProb)\n",
    "\n",
    "        Ps.append(interp(mean_R, precision, recall))\n",
    "        RPs.append(average_precision)\n",
    "\n",
    "     # 画均值\n",
    "    mean_P = np.mean(Ps, axis=0)\n",
    "    mean_RPs = np.mean(RPs, axis=0)\n",
    "    plt.plot(mean_P, mean_R,linestyle='--',color=colorlist[0],label=r'GNB(AUPR=%0.4f)'%mean_RPs,lw=1.5,alpha=1)\n",
    "\n",
    "    Ps = []\n",
    "    RPs = []\n",
    "    mean_R = np.linspace(0, 1, 1000)\n",
    "    for j in range(5):\n",
    "        # 读取文件\n",
    "        RAPNameProb = 'predict/SVC_' + str(j) + 'Prob.csv'\n",
    "        RealAndPredictionProb = pd.read_csv(RAPNameProb, header=None)\n",
    "\n",
    "        Real = RealAndPredictionProb[0]\n",
    "        PredictionProb = RealAndPredictionProb[1]\n",
    "\n",
    "        # 画图\n",
    "        average_precision = average_precision_score(Real, PredictionProb)\n",
    "        precision, recall, _ = precision_recall_curve(Real, PredictionProb)\n",
    "\n",
    "        Ps.append(interp(mean_R, precision, recall))\n",
    "        RPs.append(average_precision)\n",
    "\n",
    "     # 画均值\n",
    "    mean_P = np.mean(Ps, axis=0)\n",
    "    mean_RPs = np.mean(RPs, axis=0)\n",
    "    plt.plot(mean_P, mean_R,linestyle='-.',color=colorlist[4],label=r'SVM(AUPR=%0.4f)'%mean_RPs,lw=1.5,alpha=1)  \n",
    "      \n",
    "plt.plot([1,0],[0,1],linestyle='--',lw=2,color='rosybrown',alpha=0.8)    \n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='best')   \n",
    "plt.savefig('image/Compare_PR.svg')\n",
    "plt.savefig('image/Compare_PR.tif')\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "310.417px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
